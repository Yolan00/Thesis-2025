"""
deep_fewshot.py

Few-shot evaluation for a 4-image reference game using DeepSeek-VL2.

Compared to the baseline, this script *primes* the Speaker with example trials
sampled from other folders in the dataset, then evaluates the Listener's guess
and next-token probabilities. It repeats the whole experiment for multiple runs
(default: 10) to stabilize estimates.

Pipeline (per run):
  1) For each trial folder and each target index (1..4):
     a) FEW-SHOT SAMPLING: Randomly sample example folders (not the current one).
        Load human description(s) for a target image; show example images + gold
        description to the Speaker as demonstrations.
     b) SPEAKER: Given examples + current 4 images and a target index, generate
        a description without revealing position (no "first/second/third/fourth").
     c) LISTENER: Given the generated description + the 4 images, produce the
        guess (first/second/third/fourth). Also extract the next-token logits
        to compute cumulative probabilities over lexical variants for each option.
     d) LOGGING: Write prompts, descriptions, guesses, and probability tables
        to OUTPUT_FILE, and record descriptions into JSON at the end.
  2) SUMMARY: Compute per-run accuracy, description length stats, and ranked
     cumulative probabilities (split by correct vs wrong). After all runs,
     write an all-runs summary file with macro-level aggregates.

Inputs
------
- DATASET_DIR: Dataset root containing subfolders, each with exactly 4 images
  (default ".jpg") and one CSV named "trials_for_*.csv" with column 'msg'.
- MODEL_PATH: HF repo ID or local checkpoint for DeepSeek-VL2.

Outputs
-------
- OUTPUT_FILE (.txt): Per-trial logs and per-run summaries.
- OUTPUT_FILE.replace(".txt", "_descriptions.json"): All Speaker descriptions
  with metadata and correctness labels (per final summary block).
- OUTPUT_FILE.replace(".txt", "_allruns_summary.txt"): Macro-summary over runs.

Notes
-----
- Deterministic decoding (do_sample=False) for reproducibility.
- Listener probabilities are taken from the FIRST generated token (next-token).
- Few-shot sampling enforces examples from different folders and tries to avoid
  using the current folder; see get_few_shot_examples().

How to run
----------
$ python deep_fewshot.py

Dependencies
------------
- torch, transformers, pandas, PIL, deepseek_vl2 (DeepseekVLV2Processor, load_pil_images)
"""

import os
import statistics
import json
import random
import pandas as pd
from PIL import Image
import torch
import torch.nn.functional as F

from deepseek_vl2.models import DeepseekVLV2Processor
from transformers import AutoModelForCausalLM
from deepseek_vl2.utils.io import load_pil_images

# ----------------------------
# Configuration
# ----------------------------
DATASET_DIR = "/projects/0/prjs1482/UvA/AA_DATASET/dataset"
OUTPUT_FILE = "/projects/0/prjs1482/UvA/Outputs/PART_1/Deep/Few_Shots/deepseek_fewshot_output.txt"
MODEL_PATH = "deepseek-ai/deepseek-vl2"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_IMAGES = 4
IMAGE_EXT = ".jpg"

# ----------------------------
# Variables/statistics (accumulated within a run unless reset)
# ----------------------------
correct_count = 0
wrong_count = 0
total_count = 0
trial_index = 0
total_description_length = 0
description_lengths = []

# Max (rank-1) next-token cumulative probability when correct vs wrong
prob_correct = []
prob_wrong = []

# Ranked cumulative probabilities split by correctness
rank1_probs_correct, rank2_probs_correct, rank3_probs_correct, rank4_probs_correct = [], [], [], []
rank1_probs_wrong,   rank2_probs_wrong,   rank3_probs_wrong,   rank4_probs_wrong   = [], [], [], []

# Store per-trial speaker descriptions with metadata
speaker_desc_records = []

# ----------------------------
# Load model and processor
# ----------------------------
print("Loading DeepSeek-VL2...")
processor = DeepseekVLV2Processor.from_pretrained(MODEL_PATH)
tokenizer = processor.tokenizer
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = model.to(torch.bfloat16).to(DEVICE).eval()

# ----------------------------
# Helper functions
# ----------------------------
def load_images(image_paths):
    """Load file paths as RGB PIL images."""
    return [Image.open(p).convert("RGB") for p in image_paths]

def find_csv_file(folder):
    """Return the path to a CSV named 'trials_for_*.csv' in the given folder, if any."""
    for f in os.listdir(folder):
        if f.endswith(".csv") and f.startswith("trials_for_"):
            return os.path.join(folder, f)
    return None

def get_image_paths(folder, ext=IMAGE_EXT):
    """Return a sorted list of image file paths in a folder."""
    return sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(ext)])

def get_answer_variants():
    """Lexical variants for each answer option (including capitalization and numerals).

    These help capture single-token realizations like ' first', '1st', or '2'.
    """
    return {
        "first":  ["first", "First", "one", "One", "1", "1st", " 1", " 1st", " first", " First"],
        "second": ["second", "Second", "two", "Two", "2", "2nd", " 2", " 2nd", " second", " Second"],
        "third":  ["third", "Third", "three", "Three", "3", "3rd", " 3", " 3rd", " third", " Third"],
        "fourth": ["fourth", "Fourth", "four", "Four", "4", "4th", " 4", " 4th", " fourth", " Fourth"],
    }

def get_all_variant_token_ids(tokenizer, answer_variants):
    """Map each option to the set of *single-token* IDs for its variants.

    We encode with and without a leading space to capture tokenizers where
    ' first' and 'first' differ. Only variants that encode to exactly one token
    are kept.
    """
    variant_token_ids = {}
    for key, variants in answer_variants.items():
        token_ids = set()
        for v in variants:
            for prefix in ["", " "]:
                ids = tokenizer.encode(prefix + v, add_special_tokens=False)
                if len(ids) == 1:
                    token_ids.add(ids[0])
        variant_token_ids[key] = token_ids
    return variant_token_ids

def get_cumulative_probs(probs_full, variant_token_ids):
    """Aggregate next-token probabilities over variants to get per-option totals.

    Parameters
    ----------
    probs_full : torch.Tensor
        Softmax probabilities over the full vocab for the next token.
    variant_token_ids : dict[str, set[int]]
        Option -> token IDs (single-token variants) for that option.

    Returns
    -------
    list[tuple[str, float]]
        (option, cumulative_probability) sorted descending by probability.
    """
    cum_probs = {}
    for ans, tids in variant_token_ids.items():
        # Sum probabilities of all known single-token variants for this answer.
        prob = sum(float(probs_full[tid]) for tid in tids if tid < len(probs_full))
        cum_probs[ans] = prob
    sorted_items = sorted(cum_probs.items(), key=lambda x: -x[1])
    return sorted_items

def get_few_shot_examples(base_dir, current_folder):
    """Sample two *different-folder* example trials to use as demonstrations.

    Strategy:
      - Build a candidate pool of folders excluding the current one.
      - Repeatedly sample a folder, load its images and CSV, then randomly pick
        one target row (index 0..3) to take a human description ('msg').
      - Ensure two examples come from two different folders.

    Returns
    -------
    list[dict] | None
        Two-example list with keys: 'images', 'target_idx', 'target_label', 'desc', 'folder';
        or None if sampling fails.
    """
    try:
        folder_pool = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f)) and f != current_folder]
        examples = []
        for _ in range(2):  # sample two different examples
            example_folder = random.choice(folder_pool)
            example_path = os.path.join(base_dir, example_folder)
            example_image_paths = get_image_paths(example_path)
            csv_path = find_csv_file(example_path)
            if not csv_path or len(example_image_paths) != NUM_IMAGES:
                continue
            df = pd.read_csv(csv_path, encoding="utf-8-sig")
            if len(df) < NUM_IMAGES:
                continue
            idx = random.choice(range(NUM_IMAGES))
            desc = df.iloc[idx]['msg']
            ordinal = ['first', 'second', 'third', 'fourth'][idx]
            examples.append({
                "images": example_image_paths,
                "target_idx": idx,
                "target_label": ordinal,
                "desc": desc,
                "folder": example_folder
            })
            folder_pool.remove(example_folder)  # enforce *different* example folders
        if len(examples) == 2:
            return examples
        else:
            return None
    except Exception as e:
        print(f"Error getting few-shot examples: {e}")
        return None

# ----------------------------
# Main trial
# ----------------------------
def run_trial(trial_path, target_idx):
    """Run a single trial for a given target index (0..3) using one few-shot example block.

    Steps:
      - Validate images/CSV for current folder.
      - Sample two example folders; build a composite prompt that shows the example
        images and one gold description, then the current 4 images and the target index.
      - Generate Speaker description (deterministic).
      - Run Listener to get the guess and next-token probabilities.
      - Aggregate variant probabilities per option; record ranks and correctness.
      - Log trial details to OUTPUT_FILE and cache description metadata in memory.

    Side effects:
      - Mutates global counters/lists and appends to OUTPUT_FILE.
    """
    global trial_index, correct_count, wrong_count, total_count, total_description_length
    global prob_correct, prob_wrong, description_lengths
    global rank1_probs_correct, rank2_probs_correct, rank3_probs_correct, rank4_probs_correct
    global rank1_probs_wrong, rank2_probs_wrong, rank3_probs_wrong, rank4_probs_wrong
    global speaker_desc_records

    trial_index += 1

    # --- Validate trial folder contents ---
    img_paths = get_image_paths(trial_path)
    if len(img_paths) != NUM_IMAGES:
        print(f"Skipping {trial_path}: expected {NUM_IMAGES} images, found {len(img_paths)}")
        return

    csv_path = find_csv_file(trial_path)
    if not csv_path:
        print(f"Skipping {trial_path}: no suitable .csv file found")
        return

    # --- Few-shot sampling (exclude the current folder) ---
    fewshot_examples = get_few_shot_examples(DATASET_DIR, os.path.basename(trial_path))
    if not fewshot_examples:
        print(f"Skipping {trial_path}: could not find enough few-shot examples.")
        return

    target_label = ['first', 'second', 'third', 'fourth'][target_idx]
    target_img_name = os.path.basename(img_paths[target_idx])

    # ---------------- SPEAKER TURN (with one example block in the prompt) ----------------
    prompt = (
        "<|grounding|> You are playing a reference game as the speaker. I will give you 4 different images. "
        "Your task is to generate a description of the TARGET image so that a listener can identify it among the others. "
        "Do NOT mention the position or order of the image (e.g., 'first', 'second', etc.). "
        "Focus only on unique visual details that distinguish the target.\n"
        "Look at this example, 4 images are given,\n"
        "<image>\n"
        "<image>\n"
        "<image>\n"
        "<image>\n"
        f"Then the TARGET is selected, in this case, the {fewshot_examples[0]['target_label']} image, "
        f"And a description is generated, DESCRIPTION: {fewshot_examples[0]['desc']}\n"
        "Now it is your turn to generate a description for the target image.\n\n"
        "Those are your 4 images,\n"
        "First: <image>\n"
        "Second: <image>\n"
        "Third: <image>\n"
        "Fourth: <image>\n"
        f"Your TARGET is the {target_label} image.\n"
        f"Your DESCRIPTION of the {target_label} image:"
    )

    # Only the *first* example block's images are shown before the current 4
    all_images = []
    all_images += fewshot_examples[0]['images']
    all_images += img_paths

    conversation = [
        {
            "role": "<|User|>",
            "content": prompt,
            "images": all_images,
        },
        {"role": "<|Assistant|>", "content": ""}
    ]

    # Prepare multimodal inputs (images + text) for DeepSeek-VL2
    pil_images = load_pil_images(conversation)
    prepare_inputs = processor(
        conversations=conversation,
        images=pil_images,
        force_batchify=True,
        system_prompt=""
    ).to(DEVICE)

    # Deterministic generation of the Speaker description
    with torch.no_grad():
        inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)
        outputs = model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=prepare_inputs.attention_mask,
            pad_token_id=tokenizer.eos_token_id,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            max_new_tokens=64,
            do_sample=False,
            use_cache=True
        )
    generated_desc = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True).strip()

    # Track description length stats
    total_description_length += len(generated_desc.split())
    description_lengths.append(len(generated_desc.split()))
    raw_speaker_prompt = prompt

    # ---------------- LISTENER TURN ----------------
    listener_prompt = (
        f"You are playing a reference game as the listener. Here is a description: \"{generated_desc}\"\n"
        f"Which of the four images does this description refer to?\n"
        f"first: <image>\n second: <image>\n third: <image>\n fourth: <image>\n\n"
        f"Answer with one word: first, second, third, or fourth."
    )
    conversation_listener = [
        {
            "role": "<|User|>",
            "content": listener_prompt,
            "images": img_paths,
        },
        {"role": "<|Assistant|>", "content": ""}
    ]
    pil_images_listener = load_pil_images(conversation_listener)
    prepare_inputs_listener = processor(
        conversations=conversation_listener,
        images=pil_images_listener,
        force_batchify=True,
        system_prompt=""
    ).to(DEVICE)

    # Single call that both generates the guess and exposes next-token scores
    with torch.no_grad():
        inputs_embeds_listener = model.prepare_inputs_embeds(**prepare_inputs_listener)
        outputs_listener = model.generate(
            inputs_embeds=inputs_embeds_listener,
            attention_mask=prepare_inputs_listener.attention_mask,
            pad_token_id=tokenizer.eos_token_id,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            max_new_tokens=10,
            do_sample=False,
            use_cache=True,
            output_scores=True,
            return_dict_in_generate=True
        )

    # Listener's raw guess (text)
    listener_guess = tokenizer.decode(outputs_listener.sequences[0], skip_special_tokens=True).strip().lower()
    raw_listener_prompt = listener_prompt

    # Next-token probabilities from the FIRST generated token
    logits = outputs_listener.scores[0]
    probs_full = F.softmax(logits[0], dim=-1)

    # Aggregate across lexical variants to obtain cumulative option probabilities
    answer_variants = get_answer_variants()
    variant_token_ids = get_all_variant_token_ids(tokenizer, answer_variants)
    cum_probs_table = get_cumulative_probs(probs_full, variant_token_ids)
    max_prob = max((prob for _, prob in cum_probs_table), default=0)

    # Outcome and rank-wise bookkeeping
    correct = listener_guess == target_label
    result_text = "correct" if correct else "wrong"

    ranks = [cum_probs_table[i][1] if i < len(cum_probs_table) else 0 for i in range(4)]
    if correct:
        prob_correct.append(max_prob)
        rank1_probs_correct.append(ranks[0])
        rank2_probs_correct.append(ranks[1])
        rank3_probs_correct.append(ranks[2])
        rank4_probs_correct.append(ranks[3])
        correct_count += 1
    else:
        prob_wrong.append(max_prob)
        rank1_probs_wrong.append(ranks[0])
        rank2_probs_wrong.append(ranks[1])
        rank3_probs_wrong.append(ranks[2])
        rank4_probs_wrong.append(ranks[3])
        wrong_count += 1

    total_count += 1

    # Record description with metadata for JSON export
    speaker_desc_records.append({
        "folder": os.path.basename(trial_path),
        "target_label": target_label,
        "target_img_name": target_img_name,
        "description": generated_desc,
        "listener_guess": listener_guess,
        "correct": correct
    })

    # Console summary for the current trial
    print("\n----------------------------------------")
    print(f"Trial: {os.path.basename(trial_path)}")
    print(f"Target image: {target_idx + 1} ({target_img_name})")
    print(f"Generated description: {generated_desc}")
    print(f"Listener guess: {listener_guess}")
    print(f"Result: {result_text}")
    print("----------------------------------------\n")

    # Append detailed logs to the output text file
    with open(OUTPUT_FILE, "a") as f:
        f.write(f"=== TRIAL {trial_index} ===\n\n")
        f.write(f"FOLDER: {os.path.basename(trial_path)}\n")
        f.write(f"TARGET IMAGE: {target_img_name}, {target_label}\n\n")
        f.write("[DESCRIPTION GENERATED]\n")
        f.write(generated_desc + "\n\n")
        f.write("[LISTENER GUESS]: " + listener_guess + "\n")
        f.write("[RESULT]: " + result_text + "\n\n")
        f.write("[LISTENER NEXT-TOKEN PROBABILITIES]\n")
        f.write("Answer   | Cumulative Probability\n")
        for ans, prob in cum_probs_table:
            f.write(f"{ans:<8} | {prob:.4f}\n")
        f.write("\n")
        f.write("[RAW SPEAKER PROMPT]\n")
        f.write(raw_speaker_prompt + "\n\n")
        f.write("[RAW LISTENER PROMPT]\n")
        f.write(raw_listener_prompt + "\n\n")
        f.write("=" * 80 + "\n\n")

# ----------------------------
# Deterministic 4-pass navigation over folders and targets
# ----------------------------
if __name__ == "__main__":
    # === All-run statistics (macro across runs) ===
    all_run_accuracies = []
    all_run_avg_lens = []
    all_run_std_lens = []
    all_run_total_counts = []
    all_run_correct_counts = []
    all_run_wrong_counts = []

    all_run_rank_stats = []  # list of dicts for each run

    NUM_RUNS = 10

    for run_idx in range(NUM_RUNS):
        print(f"\n########### STARTING RUN {run_idx + 1} / {NUM_RUNS} ###########\n")
        
        # Reset per-run stats
        correct_count = 0
        wrong_count = 0
        total_count = 0
        trial_index = 0
        total_description_length = 0
        description_lengths = []

        prob_correct = []
        prob_wrong = []

        rank1_probs_correct, rank2_probs_correct, rank3_probs_correct, rank4_probs_correct = [], [], [], []
        rank1_probs_wrong,   rank2_probs_wrong,   rank3_probs_wrong,   rank4_probs_wrong   = [], [], [], []

        speaker_desc_records = []

        # Enumerate all trial folders for this run
        all_folders = [d for d in sorted(os.listdir(DATASET_DIR)) if os.path.isdir(os.path.join(DATASET_DIR, d))]

        for target_idx in range(NUM_IMAGES):  # 0,1,2,3 (first, second, third, fourth)
            print(f"\n======== CYCLE for TARGET INDEX {target_idx + 1} (RUN {run_idx + 1}) ========\n")
            for folder in all_folders:
                trial_path = os.path.join(DATASET_DIR, folder)
                run_trial(trial_path, target_idx)

        # ---- Per-run summary/statistics ----
        accuracy = correct_count / total_count * 100 if total_count > 0 else 0
        avg_len = total_description_length / total_count if total_count > 0 else 0
        std_len = statistics.stdev(description_lengths) if len(description_lengths) > 1 else 0

        # Convenience: (mean, stdev) or (0,0) if too few samples
        def _avgstd(L): return (statistics.mean(L) if L else 0, statistics.stdev(L) if len(L) > 1 else 0)
        rank_stats = {
            "rank1_correct": _avgstd(rank1_probs_correct),
            "rank2_correct": _avgstd(rank2_probs_correct),
            "rank3_correct": _avgstd(rank3_probs_correct),
            "rank4_correct": _avgstd(rank4_probs_correct),
            "rank1_wrong": _avgstd(rank1_probs_wrong),
            "rank2_wrong": _avgstd(rank2_probs_wrong),
            "rank3_wrong": _avgstd(rank3_probs_wrong),
            "rank4_wrong": _avgstd(rank4_probs_wrong),
        }

        # Persist per-run aggregates for the all-runs overview
        all_run_accuracies.append(accuracy)
        all_run_avg_lens.append(avg_len)
        all_run_std_lens.append(std_len)
        all_run_total_counts.append(total_count)
        all_run_correct_counts.append(correct_count)
        all_run_wrong_counts.append(wrong_count)
        all_run_rank_stats.append(rank_stats)

        # Per-trial logs are already written to OUTPUT_FILE

    # ---- OVERALL ALL-RUNS SUMMARY ----
    overall_output = OUTPUT_FILE.replace(".txt", "_allruns_summary.txt")
    with open(overall_output, "w") as f:
        f.write("==== SUMMARY OVER 10 RUNS ====\n")
        f.write(f"Total runs: {NUM_RUNS}\n\n")
        f.write(f"Mean accuracy: {statistics.mean(all_run_accuracies):.2f} ± {statistics.stdev(all_run_accuracies):.2f}%\n")
        f.write(f"Mean desc len: {statistics.mean(all_run_avg_lens):.2f} ± {statistics.stdev(all_run_avg_lens):.2f} words\n")
        f.write(f"Mean std desc len: {statistics.mean(all_run_std_lens):.2f} ± {statistics.stdev(all_run_std_lens):.2f} words\n\n")
        f.write(f"Total trials (all runs): {sum(all_run_total_counts)}\n")
        f.write(f"Mean correct per run: {statistics.mean(all_run_correct_counts):.1f}\n")
        f.write(f"Mean wrong per run: {statistics.mean(all_run_wrong_counts):.1f}\n")
        f.write("\nPer-run accuracies:\n")
        for i, acc in enumerate(all_run_accuracies):
            f.write(f"Run {i+1}: {acc:.2f}%\n")

        f.write("\n==== Rank statistics per run ====\n")
        for i, rs in enumerate(all_run_rank_stats):
            f.write(f"\nRun {i+1}:\n")
            for key, val in rs.items():
                f.write(f"{key}: {val[0]:.4f} ± {val[1]:.4f}\n")

    print("ALL RUNS COMPLETE. See per-run output in your output file and all-runs summary at:", overall_output)
