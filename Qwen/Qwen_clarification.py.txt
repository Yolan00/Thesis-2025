"""
Qwen_clarification.py

Uncertain-Listener + Feedback (Clarification) loop for a 4-image reference game
using Qwen2.5-VL-7B-Instruct.

Overview
--------
This script takes *baseline* speaker descriptions (produced earlier and saved to JSON)
and runs a two-stage evaluation per trial:

  1) ROUND 1 — Listener on the baseline description:
     - The Listener reads the description and sees the 4 images.
     - It produces a single-word guess: "first", "second", "third", or "fourth".
     - We extract the FIRST generated token's logits (next-token) and compute a
       cumulative probability for each option by summing over lexical variants (e.g.,
       " first", "1st", " 1").

  2) UNCERTAINTY CHECK + ROUND 2 — Feedback:
     - If the maximum cumulative probability across options is <= UNCERTAINTY_THRESHOLD,
       the Listener is considered uncertain.
     - We create a *subset* consisting of all options whose cumulative prob >= PROB_SUBSET,
       and **always include the true target**.
     - We force the target to be **IMAGE 1** within this subset and ask the Speaker to
       generate a refined description of IMAGE 1, given the old description.
     - The Listener then re-evaluates with the **full set of 4 images** using the new description.

All per-trial details (prompts, guesses, probability tables) are appended to a text file,
and structured results are saved to JSON. A final statistics section summarizes:
trigger rate of feedback, round-2 accuracy, flips (Correct→Wrong, Wrong→Correct),
and feedback description lengths.

Inputs
------
- DATASET_DIR     : Root folder containing trial subfolders with 4 images ('.jpg').
- JSON_BASELINE   : JSON file of baseline speaker outputs with fields:
                    folder, target_label, description, target_img_name, correct.
- MODEL_NAME      : HF model id for Qwen2.5-VL-7B-Instruct.

Outputs
-------
- OUTPUT_FILE (.txt) : Human-readable per-trial logs + statistics summary.
- OUTPUT_JSON (.json): Structured results (first and second round info for each trial).

Notes
-----
- Deterministic decoding (do_sample=False).
- Next-token probabilities derive from the FIRST generated token only.
- Subset selection always includes the true target; target is fixed as IMAGE 1 in
  the feedback subset to simplify the Speaker prompt.

How to run
----------
$ python Qwen_clarification.py

Dependencies
------------
- torch, transformers (AutoProcessor, Qwen2_5_VLForConditionalGeneration)
- PIL, qwen_vl_utils.process_vision_info
"""

import os
import json
import statistics
from PIL import Image
import torch
import torch.nn.functional as F
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from qwen_vl_utils import process_vision_info

# ----------------------------
# Configuration
# ----------------------------
DATASET_DIR = "/projects/0/prjs1482/UvA/AA_DATASET/dataset_blur_25"
JSON_BASELINE = "/projects/0/prjs1482/UvA/Outputs/Blur/qwen_BLUR25_output_descriptions.json"
OUTPUT_FILE = "/projects/0/prjs1482/UvA/Outputs/PART_1/Qwen/Multiple_Answers/qwen_BLUR25_feedback_output.txt"
OUTPUT_JSON = "/projects/0/prjs1482/UvA/Outputs/PART_1/Qwen/Multiple_Answers/qwen_BLUR25_feedback_output.json"
MODEL_NAME = "Qwen/Qwen2.5-VL-7B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_IMAGES = 4
IMAGE_EXT = ".jpg"

# Uncertainty thresholds
UNCERTAINTY_THRESHOLD = 0.78  # trigger feedback when max cumulative prob <= this
PROB_SUBSET = 0.10            # include any option with cumulative prob >= this in subset

# ----------------------------
# Load model and processor
# ----------------------------
print("Loading Qwen2.5-VL-7B-Instruct...")
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=torch.bfloat16
)
processor = AutoProcessor.from_pretrained(MODEL_NAME)
model.eval()

# ----------------------------
# Helper functions
# ----------------------------
def load_images(image_paths):
    """Open a list of image paths as RGB PIL images."""
    return [Image.open(p).convert("RGB") for p in image_paths]

def get_image_paths(folder, ext=IMAGE_EXT):
    """Return a sorted list of image paths under a folder with the given extension."""
    return sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(ext)])

def get_answer_variants():
    """Lexicalized answer variants for each option (captures single-token realizations)."""
    return {
        "first":  ["first", "First", "one", "One", "1", "1st", " 1", " 1st", " first", " First"],
        "second": ["second", "Second", "two", "Two", "2", "2nd", " 2", " 2nd", " second", " Second"],
        "third":  ["third", "Third", "three", "Three", "3", "3rd", " 3", " 3rd", " third", " Third"],
        "fourth": ["fourth", "Fourth", "four", "Four", "4", "4th", " 4", " 4th", " fourth", " Fourth"],
    }

def get_all_variant_token_ids(processor, answer_variants):
    """Map options to the set of *single-token* IDs for their lexical variants.

    We encode with and without a leading space to capture tokenizers where ' first'
    and 'first' differ. Only variants that encode to exactly one token are retained.
    """
    variant_token_ids = {}
    for key, variants in answer_variants.items():
        token_ids = set()
        for v in variants:
            for prefix in ["", " "]:
                ids = processor.tokenizer.encode(prefix + v, add_special_tokens=False)
                if len(ids) == 1:
                    token_ids.add(ids[0])
        variant_token_ids[key] = token_ids
    return variant_token_ids

def get_cumulative_probs(probs_full, variant_token_ids):
    """Aggregate next-token probabilities over variants to get per-option totals.

    Parameters
    ----------
    probs_full : torch.Tensor
        Softmax probabilities over the full vocab for the FIRST generated token.
    variant_token_ids : dict[str, set[int]]
        Option -> token IDs representing single-token variants for that option.

    Returns
    -------
    list[tuple[str, float]]
        (option, cumulative_probability) sorted descending by probability.
    """
    cum_probs = {}
    for ans, tids in variant_token_ids.items():
        # Sum probabilities of all known single-token variants for this answer
        prob = sum(float(probs_full[tid]) for tid in tids if tid < len(probs_full))
        cum_probs[ans] = prob
    sorted_items = sorted(cum_probs.items(), key=lambda x: -x[1])
    return sorted_items

def run_listener(desc, listener_img_paths):
    """Run the Listener on a description + 4 images.

    Returns
    -------
    tuple
        (listener_guess: str, probs_full: torch.Tensor, listener_prompt: str, raw_text_prompt: str)
    """
    listener_images = load_images(listener_img_paths)
    listener_prompt = (
        f"You are playing a reference game as the listener. Here is a description: \"{desc}\"\n"
        f"Which of the four images does this description refer to?\n"
        f"Answer with one word: first, second, third, or fourth."
    )

    # Construct a multimodal message (4 images + prompt text)
    listener_messages = [
        {
            "role": "user",
            "content": (
                [{"type": "image", "image": img} for img in listener_images] +
                [{"type": "text", "text": listener_prompt}]
            )
        }
    ]
    listener_text_prompt = processor.apply_chat_template(
        listener_messages, tokenize=False, add_generation_prompt=True
    )
    listener_image_inputs, listener_video_inputs = process_vision_info(listener_messages)

    # Prepare inputs for Qwen
    listener_inputs = processor(
        text=[listener_text_prompt],
        images=listener_image_inputs,
        videos=listener_video_inputs,
        return_tensors="pt",
        padding=True
    ).to(DEVICE)

    # Deterministic generation with next-token scores exposed
    with torch.no_grad():
        listener_outputs = model.generate(
            **listener_inputs,
            max_new_tokens=10,
            do_sample=False,
            output_scores=True,
            return_dict_in_generate=True
        )

    # FIRST generated token probabilities
    logits = listener_outputs.scores[0][:, :]
    probs_full = F.softmax(logits[0], dim=-1)

    # Decode the listener guess (text generated after the prompt)
    trimmed_listener = listener_outputs.sequences[:, listener_inputs.input_ids.shape[1]:]
    listener_guess = processor.batch_decode(trimmed_listener, skip_special_tokens=True)[0].strip().lower()
    return listener_guess, probs_full, listener_prompt, listener_text_prompt

# ----------------------------
# Main experiment
# ----------------------------
if __name__ == "__main__":
    # Load baseline speaker outputs (descriptions with metadata)
    with open(JSON_BASELINE, "r") as f:
        baseline_results = json.load(f)

    # Prepare output files
    if os.path.exists(OUTPUT_FILE):
        os.remove(OUTPUT_FILE)
    feedback_results = []

    answer_variants = get_answer_variants()
    variant_token_ids = get_all_variant_token_ids(processor, answer_variants)

    for trial in baseline_results:
        folder = trial["folder"]
        target_label = trial["target_label"]
        description = trial["description"]
        target_img_name = trial["target_img_name"]
        correct_first = trial["correct"]  # baseline R1 correctness (informational)

        # Load the 4 image paths for this trial
        speaker_trial_path = os.path.join(DATASET_DIR, folder)
        img_paths = get_image_paths(speaker_trial_path)
        idx_target = ['first', 'second', 'third', 'fourth'].index(target_label)

        # ------------- ROUND 1: Listener on the baseline description -------------
        listener_guess, probs_full, listener_prompt, raw_listener_prompt = run_listener(description, img_paths)
        cum_probs_table = get_cumulative_probs(probs_full, variant_token_ids)
        max_prob = cum_probs_table[0][1]  # rank-1 cumulative probability

        # Record first-round info for this trial
        trial_out = {
            "folder": folder,
            "target_label": target_label,
            "target_img_name": target_img_name,
            "first_round": {
                "description": description,
                "listener_guess": listener_guess,
                "max_prob": max_prob,
                "prob_table": cum_probs_table,
                "correct": listener_guess == target_label,
                "raw_listener_prompt": raw_listener_prompt,
            }
        }

        # Append ROUND 1 log to OUTPUT_FILE
        with open(OUTPUT_FILE, "a") as f:
            f.write(f"=== TRIAL [{folder}] Target: {target_label} ({target_img_name}) ===\n")
            f.write("[ROUND 1]\n")
            f.write(f"Description: {description}\n")
            f.write(f"Listener guess: {listener_guess}\n")
            f.write(f"Probabilities: {cum_probs_table}\n")
            f.write(f"Result: {'correct' if listener_guess == target_label else 'wrong'}\n")
            f.write(f"Max prob: {max_prob:.4f}\n")

        # ------------- ROUND 2: Feedback (only if uncertain) -------------
        feedback_info = None
        if max_prob <= UNCERTAINTY_THRESHOLD:
            # Build subset: options with probability >= PROB_SUBSET
            feedback_indices = [i for i, (ans, prob) in enumerate(cum_probs_table) if prob >= PROB_SUBSET]
            feedback_labels = [cum_probs_table[i][0] for i in feedback_indices]
            feedback_img_indices = [['first', 'second', 'third', 'fourth'].index(lbl) for lbl in feedback_labels]

            # Ensure the true target is present in the subset
            if idx_target not in feedback_img_indices:
                feedback_img_indices.append(idx_target)

            # Always put the true target FIRST (IMAGE 1) in the subset
            if feedback_img_indices[0] != idx_target:
                tgt_idx_pos = feedback_img_indices.index(idx_target)
                feedback_img_indices[0], feedback_img_indices[tgt_idx_pos] = (
                    feedback_img_indices[tgt_idx_pos], feedback_img_indices[0]
                )

            subset_imgs = [img_paths[i] for i in feedback_img_indices]

            # Speaker feedback prompt (explicitly states TARGET is IMAGE 1)
            feedback_prompt = (
                "You are playing the second round of a reference game as the speaker.\n "
                f"The listener is still unsure about which image is the target and has selected {len(subset_imgs)} possible targets, asking you which one you are referring to.\n"
                "Your target is IMAGE 1.\n"
                "Given your previous description, generate a new description of IMAGE 1 so that the listener can identify it among the others.\n "
                "DO NOT mention the order or position. To generate the description, use the unique elements of IMAGE 1 that distinguish it from the other images.\n"
            )

            # Construct feedback message: include subset images and the *old* description
            feedback_images = load_images(subset_imgs)
            feedback_messages = [
                {
                    "role": "user",
                    "content": (
                        [{"type": "text", "text": feedback_prompt}] +
                        [{"type": "text", "text": "\n### IMAGE SET:"}] +
                        [
                            item
                            for i, img in enumerate(feedback_images)
                            for item in (
                                [{"type": "text", "text": f"IMAGE {i+1}:"}] +
                                [{"type": "image", "image": img}]
                            )
                        ] +
                        [{"type": "text", "text": f"Old Description: {description}"}] +
                        [{"type": "text", "text": "TARGET: IMAGE 1"}] +
                        [{"type": "text", "text": "DESCRIPTION:"}]
                    )
                }
            ]
            feedback_text_prompt = processor.apply_chat_template(
                feedback_messages, tokenize=False, add_generation_prompt=True
            )
            feedback_image_inputs, feedback_video_inputs = process_vision_info(feedback_messages)

            # Generate refined description for IMAGE 1
            feedback_inputs = processor(
                text=[feedback_text_prompt],
                images=feedback_image_inputs,
                videos=feedback_video_inputs,
                return_tensors="pt",
                padding=True
            ).to(DEVICE)
            feedback_output_ids = model.generate(**feedback_inputs, max_new_tokens=80, do_sample=False)
            trimmed = feedback_output_ids[:, feedback_inputs.input_ids.shape[1]:]
            feedback_desc = processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()

            # Listener round 2 evaluates ALL original 4 images again
            listener_guess2, probs_full2, listener_prompt2, raw_listener_prompt2 = run_listener(feedback_desc, img_paths)
            cum_probs_table2 = get_cumulative_probs(probs_full2, variant_token_ids)
            max_prob2 = cum_probs_table2[0][1]
            feedback_correct = listener_guess2 == target_label

            feedback_info = {
                "feedback_prompt": feedback_prompt,
                "feedback_images": [os.path.basename(p) for p in subset_imgs],
                "feedback_desc": feedback_desc,
                "listener_guess": listener_guess2,
                "max_prob": max_prob2,
                "prob_table": cum_probs_table2,
                "correct": feedback_correct,
                "raw_feedback_prompt": feedback_text_prompt,
                "raw_listener_prompt": raw_listener_prompt2
            }
            trial_out["second_round"] = feedback_info

            # Append ROUND 2 log
            with open(OUTPUT_FILE, "a") as f:
                f.write("[ROUND 2 - FEEDBACK]\n")
                f.write(f"Subset images: {[os.path.basename(p) for p in subset_imgs]}\n")
                f.write(f"Feedback prompt: {feedback_prompt}\n")
                f.write(f"RAW SPEAKER PROMPT:\n{feedback_text_prompt}\n")
                f.write(f"Generated feedback description: {feedback_desc}\n")
                f.write(f"Listener guess (round 2): {listener_guess2}\n")
                f.write(f"Probabilities (round 2): {cum_probs_table2}\n")
                f.write(f"Result (round 2): {'correct' if feedback_correct else 'wrong'}\n")
                f.write(f"Max prob (round 2): {max_prob2:.4f}\n")
                f.write("-"*80 + "\n")
        else:
            # No feedback triggered
            with open(OUTPUT_FILE, "a") as f:
                f.write("No feedback round triggered.\n")
                f.write("-"*80 + "\n")

        feedback_results.append(trial_out)

    # ----------------------------
    # Statistics over all trials
    # ----------------------------
    n_total = 0
    n_feedback = 0
    n_correct_r1 = 0
    n_correct_r2 = 0
    n_r1_to_r2_wrong = 0
    n_r1_to_r2_right = 0
    feedback_desc_lengths = []

    for trial in feedback_results:
        n_total += 1
        r1 = trial['first_round']
        r1_correct = r1['correct']
        n_correct_r1 += int(r1_correct)

        has_fb = 'second_round' in trial and trial['second_round'] is not None
        if has_fb:
            n_feedback += 1
            r2 = trial['second_round']
            r2_correct = r2['correct']
            n_correct_r2 += int(r2_correct)

            # Feedback description length for round 2
            if 'feedback_desc' in r2:
                feedback_desc_lengths.append(len(r2['feedback_desc'].split()))

            # Flip counters
            if r1_correct and not r2_correct:
                n_r1_to_r2_wrong += 1
            if (not r1_correct) and r2_correct:
                n_r1_to_r2_right += 1

    # Feedback description length summary
    if feedback_desc_lengths:
        avg_fb_len = statistics.mean(feedback_desc_lengths)
        std_fb_len = statistics.stdev(feedback_desc_lengths) if len(feedback_desc_lengths) > 1 else 0
    else:
        avg_fb_len = 0
        std_fb_len = 0

    # Round-2 accuracy (only among trials that triggered feedback)
    acc_r2 = 100 * n_correct_r2 / n_feedback if n_feedback else 0

    # Append statistics to the output text file
    with open(OUTPUT_FILE, "a") as f:
        f.write("\n" + "="*28 + " FEEDBACK STATISTICS " + "="*28 + "\n")
        f.write(f"Total trials: {n_total}\n")
        f.write(f"Trials with feedback (round 2): {n_feedback} ({100*n_feedback/n_total:.2f}%)\n")
        f.write(f"Accuracy in round 2: {acc_r2:.2f}% ({n_correct_r2}/{n_feedback})\n")
        f.write(f"Round 2 correct: {n_correct_r2}\n")
        f.write(f"Round 2 wrong: {n_feedback - n_correct_r2}\n")
        f.write(f"Correct→Wrong flips: {n_r1_to_r2_wrong}\n")
        f.write(f"Wrong→Correct flips: {n_r1_to_r2_right}\n")
        f.write(f"Average feedback description length: {avg_fb_len:.2f} ± {std_fb_len:.2f} words\n")
        f.write("="*70 + "\n\n")

    # Save structured JSON output
    with open(OUTPUT_JSON, "w") as jf:
        json.dump(feedback_results, jf, indent=2)

    print("DONE. All results saved.")
